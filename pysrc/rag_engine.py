
# -----------------------------------------
# Imports
# -----------------------------------------

from langchain.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import logging

# -----------------------------------------
# Logger Setup
# -----------------------------------------

logger = logging.getLogger(__name__)

# -----------------------------------------
# RAGEngine Class Definition
# -----------------------------------------

class RAGEngine:
    """
    Class to handle the Retrieval-Augmented Generation (RAG) process,
    integrating a Large Language Model (LLM) with a vector store for question answering.

    Attributes:
        vector_store: The vector store containing document embeddings.
        temperature (float): The temperature parameter for the LLM (controls randomness).
        context_length (int): The number of context documents to retrieve.
        llm: The initialized language model.
        qa_chain: The question-answering chain combining the retriever and the LLM.
    """

    def __init__(self, vector_store):
        """
        Initialize the RAGEngine with a given vector store.

        Args:
            vector_store: The vector store containing the document embeddings.
        """
        self.vector_store = vector_store
        self.temperature = 0.7       # Default temperature
        self.context_length = 3      # Default number of context documents
        self.llm = self._initialize_llm()
        self.qa_chain = self._setup_qa_chain()

    def _initialize_llm(self):
        """
        Initialize the Large Language Model (LLM) with the specified parameters.

        Returns:
            Ollama: An instance of the Ollama language model.
        """
        return Ollama(
            model="mistral",              # Specify the model to use
            temperature=self.temperature  # Set the temperature for response randomness
        )

    def _setup_qa_chain(self):
        """
        Set up the question-answering (QA) chain that integrates the retriever and the LLM.

        Returns:
            RetrievalQA: An instance of the RetrievalQA chain.
        """
        # Define the prompt template for the LLM
        prompt_template = """
        Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.

        Context: {context}

        Question: {question}

        Answer: Let me help you with that.
        """

        # Create a PromptTemplate object
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        # Create the RetrievalQA chain
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",     # Chain type that uses all retrieved documents
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": self.context_length}  # Number of documents to retrieve
            ),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True  # Include source documents in the response
        )

    def update_settings(self, temperature: float, context_length: int):
        """
        Update the LLM settings and re-initialize the LLM and QA chain.

        Args:
            temperature (float): The new temperature for the LLM.
            context_length (int): The new number of context documents to retrieve.
        """
        self.temperature = temperature
        self.context_length = context_length
        self.llm = self._initialize_llm()
        self.qa_chain = self._setup_qa_chain()

    def answer_question(self, question: str) -> str:
        """
        Answer a question using the QA chain.

        Args:
            question (str): The question to answer.

        Returns:
            str: The answer generated by the LLM.

        Raises:
            Exception: Any exception that occurs during question answering.
        """
        try:
            # Get the response from the QA chain
            response = self.qa_chain({"query": question})
            return response["result"]
        except Exception as e:
            logger.error(f"Error generating answer: {str(e)}")
            raise
