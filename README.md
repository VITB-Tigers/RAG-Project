# ğŸ¤– RAG with Mistral - Local LLM Question Answering System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B.svg)](https://streamlit.io/)

A powerful Retrieval-Augmented Generation (RAG) system built with Mistral LLM, offering local document processing and intelligent question answering capabilities.

## ğŸ“‘ Table of Contents

- [Features](#-features)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Learning Resources](#-learning-resources)
- [Contributing](#-contributing)
- [Troubleshooting](#-troubleshooting)
- [Contact](#-contact)
- [License](#-license)

## âœ¨ Features

- ğŸ“š Local document processing (PDF, TXT, DOCX)
- ğŸ” Intelligent text chunking and embedding
- ğŸ’¡ Context-aware question answering
- ğŸ¯ Customizable response parameters
- ğŸ–¥ï¸ User-friendly Streamlit interface

## ğŸ”§ Prerequisites

- Python 3.8 or higher
- Git
- [Ollama](https://ollama.ai/) with Mistral model installed

## ğŸ“¥ Installation

1. Clone the repository:
```bash
git clone https://github.com/Preprod-Labs/DIY-RAG-LLMs.git
```

2. Create a virtual environment. If using Conda:
```
conda create -n *env_name* python==3.12.0 -y
conda activate *env_name*
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install Mistral model in Ollama:
```bash
ollama pull mistral
```

## ğŸš€ Usage

1. Start the Streamlit application:
```bash
streamlit run stmain.py
```

2. Access the web interface at `http://localhost:8501`

3. Upload documents and start asking questions!

### Basic Workflow:
1. Navigate to the "Documents" tab
2. Upload your documents
3. Process the documents
4. Switch to the "Chat" tab
5. Start asking questions about your documents

## ğŸ“ Project Structure

```
RAG_Opensrc/
    â”œâ”€â”€ pysrc/
    â”‚   â”œâ”€â”€ document_processor.py
    â”‚   â”œâ”€â”€ embeddings_manager.py
    â”‚   â””â”€â”€ rag_engine.py
    â”œâ”€â”€ stmain.py
    â”œâ”€â”€ Info.md
    â”œâ”€â”€ LearnWithPrompts.md
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md
```

## ğŸ“š Documentation

For detailed information about the project architecture and implementation details, please refer to:
- [Info.md](Info.md) - Technical documentation and system architecture
- [LearnWithPrompts.md](LearnWithPrompts.md) - Interactive learning guide with examples

## ğŸ“– Learning Resources

New to RAG systems? Check out our learning resources:
1. Review [LearnWithPrompts.md](LearnWithPrompts.md) for step-by-step tutorials
2. Explore the [Info.md](Info.md) for technical insights
3. Try the example prompts provided in the documentation


## â— Troubleshooting

Common issues and solutions:

1. **Ollama Connection Error**
   - Ensure Ollama is running locally
   - Verify Mistral model is installed

2. **Document Processing Issues**
   - Check file formats (PDF, TXT, DOCX only)
   - Ensure files are not corrupted
